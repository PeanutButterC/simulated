{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from paths import SIMULATED_PATH, SIMULATED_DATA_ROOT\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = tf.keras.datasets.mnist.load_data()\n",
    "x = {\n",
    "    'train': mnist_data[0][0],\n",
    "    'test': mnist_data[1][0]\n",
    "}\n",
    "y = {\n",
    "    'train': mnist_data[0][1],\n",
    "    'test': mnist_data[1][1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data):\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    for i in range(9):\n",
    "        ax[i // 3, i % 3].imshow(data[i])\n",
    "        ax[i // 3, i % 3].set_xticks([])\n",
    "        ax[i // 3, i % 3].set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(x['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/999\n",
      "938/938 [==============================] - 6s 4ms/step - loss: 8.0353 - accuracy: 0.8144 - val_loss: 0.5631 - val_accuracy: 0.8903\n",
      "Epoch 2/999\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.4155 - accuracy: 0.9110 - val_loss: 0.4042 - val_accuracy: 0.9178\n",
      "Epoch 3/999\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.2792 - accuracy: 0.9297 - val_loss: 0.3183 - val_accuracy: 0.9328\n",
      "Epoch 4/999\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.2186 - accuracy: 0.9425 - val_loss: 0.2776 - val_accuracy: 0.9354\n",
      "Epoch 5/999\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.2085 - accuracy: 0.9454 - val_loss: 0.2885 - val_accuracy: 0.9459\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255bdd26df0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=1)\n",
    "]\n",
    "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.fit(x['train'], y['train'], validation_data=(x['test'], y['test']), epochs=999, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "temperature = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(x):\n",
    "    a, b = tf.split(x, 2, axis=0)\n",
    "    if np.random.rand() < .75:\n",
    "        a = gaussian_filter(a, sigma=np.random.rand())\n",
    "    if np.random.rand() < .75:\n",
    "        b = gaussian_filter(b, sigma=np.random.rand())\n",
    "    return tf.concat([a, b], axis=0)\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1 / 255.)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=(-.1, .1), width_factor=(-.1, .1)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(.1)\n",
    "    #tf.keras.layers.Lambda(blur)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pairs = {}\n",
    "for type in ['train', 'test']:\n",
    "    x_pairs[type] = []\n",
    "    for i in range(0, x[type].shape[0] - batch_size + 1, batch_size):\n",
    "        x_batch = rescale(x[type][i:i+batch_size])\n",
    "        x_batch = tf.convert_to_tensor(x_batch)\n",
    "        x_batch = tf.repeat(x_batch[:, :, :, tf.newaxis], 3, axis=-1)\n",
    "        x_batch = tf.tile(x_batch, [2, 1, 1, 1])\n",
    "        x_pairs[type].append(x_batch)\n",
    "    x_pairs[type] = tf.stack(x_pairs[type])\n",
    "    x_pairs[type] = tf.random.shuffle(x_pairs[type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAADrCAYAAAAxO7C0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASm0lEQVR4nO3dWWyWVRPA8VP2pQhlbdgR2aUCfpBWQVKkoqAFN9xiBLlRLzRGjYmJwSsvvCAuCTFqTBQ1KohssskiCAjIEpS9UnZkKTsUCpR+N1/ONzO02GLb953y/13NybA8JA+Tc07PmSelpKQkAECyq5XoBwCA8qBYAXCBYgXABYoVABcoVgBcoFgBcKFORX5xSkoK5xySR0FJSUmrRD9ETcB7nVTKfK8rVKwqIiUlJcaJPstVq1atUuMQrn224uLianmmSrA30Q8AVIEy32uWgQBcoFgBcKHKloGJXPqlpqaqccOGDWOckZGhcjt27FDjo0ePxvjSpUtV8HQAbgQzKwAuUKwAuFBly8CqZn+q17p16xhnZmaq3OjRo2PcqFEjlTtw4IAaT5w4McYsA4HkwcwKgAsUKwAuUKwAuOBqz6p+/foxbtGihcqNHz8+xm+88YbKHT9+PMbbtm1TuZkzZ6pxUVHRv35OAJWPmRUAFyhWAFxI6mVgnTr68dq0aRNju9QbNmxYjPPy8lRu+vTpMf74449VrrCwUI0vX758Yw8LJDHZWEDGpY0leRPF3kqp7lsqzKwAuECxAuACxQqAC0m3Z1W7du0Yt2vXTuWysrJi3KdPH5U7c+ZMjN977z2VmzdvXoztFZqrV6/e+MMCCZSenh7j5s2bq1zbtm3V+Jlnnolxv379VK5jx44xtvvEy5cvj/HixYtVbsaMGTHes2dPOZ/6xjGzAuACxQqAC0m3DExLS4txdna2yr366qsxvnDhgsrJbgmrVq1SuYsXL1bmIwIJ0aVLFzWeNGlSjEeOHKlytiuJ3O6wRw5kw8nGjRurXE5OTowHDRqkcvn5+TFmGQgA/0OxAuACxQqAC0m3Z1W3bt0Y5+bmqpw8yrBp0yaVO3fuXIztflYyfcMQuFEjRoxQ4759+8b4/PnzKjd79mw1/uyzz2K8c+dOlZP/X+TRITu2/3fOnj1bnseuNMysALhAsQLgQtItA+WSbd++fWX+ut69e6ux7Logm+2FcO23AQEv5LaI7DoSgv74iW0aOXfuXDVevXp1jL12FmFmBcAFihUAFyhWAFxIuj2rw4cPx3jLli0qJ4/0FxcXq5z8Eetff/2lchxXgFfPPfdcjJ9++mmVk9fIUlNTVa5nz55q3KxZsxgfO3asMh+x2jCzAuACxQqAC0m3DJRHFwoKClROTnXlRyBCCOGDDz6I8ZUrV6ro6YCqZbse3H333TG2jSPffPPNGNujCvbX1gTMrAC4QLEC4ALFCoALSbdnJY8k2GsBp0+fjrG8hhCC7oxoP9rI0QV4MW7cODUeMmRIjGfNmqVy69evj3FN3KOymFkBcIFiBcCFpFsGSvYkujyxK0/khqBvoNuuC9djG+vbJaRkT80DlUF+4+/ee+9VOfler1ixQuUOHjxYtQ+WZJhZAXCBYgXABYoVABeSes+qTh39ePIIQq9evVSuSZMmMbZN7+U+VNOmTVWuXr16aty5c+cY2w9PyAb5sjtECCEUFhaW+pzAP5Ef8+3evbvKTZ48OcbLli1TOfnh0psBMysALlCsALiQ1MvAEydOqHFeXl6M27dvr3L169ePsT3d3q1btxj369dP5e6//341lp0dOnXqpHJLly4tNQ4hhFWrVsXYHp1gWQgpKytLjV944YUY2yM527Zti7G8wXEzYmYFwAWKFQAXKFYAXEjqPSv74UZ5PKBDhw4qN3DgwBjb4wiPPvpojF966SWV2717txofPXo0xlu3blW54cOHx9jufckPqZ46dUrl6FwKuac6ePBglZN7o3PmzFG5/Pz8GKenp6uc7Cp6/vx5lbNdduVVNa+YWQFwgWIFwAWKFQAXknrPyl63kXs/8npNCCFkZGTEePTo0SrXo0ePGNt9qGnTpqnx559/XurvCyGEiRMnlvn39+7dO8a2tQ2QmZkZ4zFjxqicbBEzYcIElZPvst37TEtLi7E9g7Vu3To1njJlSowXLVqkcnaPNVkxswLgAsUKgAtJvQy0123sWBo/fnyMDx06pHLyysL777+vcosXL1Zj+SNme/VBTtd37typctu3b4/x9bqN4uYkjxLY9/PIkSPl+jPseyWXfrbTiO04es8998RYbmeEEMIXX3xR6p+ZbJhZAXCBYgXABYoVABeSes/KHl3Ys2dPjO0HUOUekr2yIPep/unLN3Ltb7uRNmzYMMYHDhxQObnvwPUaWFu2bInx2LFjy/x1N9pOqEGDBmo8YMAANX777bdj/Nhjj5X5bLYbaTK9y8ysALhAsQLgQlIvA+0HGzZt2hTjDz/8UOXkEnHWrFnl/jvsj3zl0u/2229XOXlzXR6HCCGES5culfvvxM2tKjrH2q4KsltDCCGcOXMmxnfccYfKya6717s1kmjMrAC4QLEC4ALFCoALSb1nZcl9ItvZoLydEGvV0vW5ZcuWajxkyJAY2x//LlmyJMbyikIIuospUN1ycnLU+LXXXlPju+66K8abN29WuV27dsU4mTuKMrMC4ALFCoALrpaBxcXFpcb/RC795EdMQwjh8ccfV+OXX345xhs2bFC5GTNmxDiZfqSLmuN6HTvkx3pDCCE7OzvGL774osrJZpAh6CaT9tjPxo0bK/ycicDMCoALFCsALlCsALjgas+qvOwNdLlPZdf9w4YNU2PZjXT27NkqJ48uVMWVCdQctmPH0KFDY2w/rLt3794Y5+bmqtyTTz4Z4y5duqic7AJiP3L65ZdfqvGkSZNibK+KeXmXmVkBcIFiBcCFGrMMlB96GDRokMp17949xiNGjFA5+yPejz76KMZ2GQiUV1ZWlhq/8sorMe7atWu5/xz5oYlVq1ap3MKFC2M8ffp0ldu/f3+5/w4vmFkBcIFiBcAFihUAF2rMnlWTJk1i3KdPH5V7/fXXy/x98+fPV2PZMP96H1UFrqd///5qLD+Qa/eT1q5dG+MFCxaonNynkkccQgihqKjoXz+nJ8ysALhAsQLgQkpFTq+mpKS4OOr6/PPPq/Gzzz4bY/lhiRBCePfdd9U4Ly+vyp6rkq0vKSn5T6Ifoiaoivf6et0TLC8nyKtJme81MysALlCsALhAsQLgQo05uiD9+OOPanz27NkYT506tbofBzch9qEqHzMrAC5QrAC4UCOPLtwkOLpQSXivkwpHFwD4RrEC4ALFCoALFT26UBBC2PuPvwrVoVOiH6AG4b1OHmW+1xXaYAeARGEZCMAFihUAFyhWAFygWAFwgWIFwAWKFQAXKFYAXKBYAXCBYgXABYoVABcoVgBcoFgBcIFiBcAFihUAFyhWAFygWAFwgWIFwIUKtTXmk0VJpaCkpKRVoh+iJuC9TiplvtfMrPyiZzhqojLfa4oVABcoVgBcoFgBcIFiBcAFihUAFyhWAFygWAFwgWIFwAWKFQAXKFYAXKBYAXCBYgXAhQp1XaiJUlJS1Lik5P8X8FNTU1Xu3Llz1fJMAK7FzAqACxQrAC5QrAC4cNPtWbVv316N09PT1fjWW2+NccOGDcv8c+z+1fz582N8/vz5f/OIQLnUqvX/uUadOvq/cuPGjdU4LS0txvXq1VM5+b6eOHGizFyiMbMC4ALFCoALNWYZWL9+/Ri3aqX7zWdnZ8e4f//+KtevXz81btGiRYxr166tcnXr1o3x0aNHVS4nJyfGc+fOVbl58+bF+PLly6X/A4B/IN/xEPR7LrcvQrj2PZd5u0Q8fvx4jLds2aJyv/32W4x3796tcleuXCnPY1caZlYAXKBYAXCBYgXABbd7Vnb9npubG+NRo0apXN++fWNsf2zbqFEjNc7Pz4/xLbfconLHjh2L8aBBg1QuIyMjxlu3blU5eW3n5MmTASgvub/Uo0cPlRs6dGiMR44cqXJ2D0vuldojOfI937lzp8qdOXMmxocOHVI59qwAoBQUKwAuuFoGyuXUwIEDVe6dd96JcceOHVXul19+ifHKlStVzi7Z5LT37NmzKjdkyJAY9+rVq8znLCoqUuNTp06V+WsB2fmjadOmKpeZmRljudURQgiDBw+Osd2ysO/1rl27yvw7hg8fHmO7LSKXoVevXi39H1BNmFkBcIFiBcAFihUAF1ztWT388MMxnjBhgsq1bt06xvJ6SwghfPrppzFes2aNyl28eFGN5Y9jZdfQEEK48847Y1xcXKxyv//+e4z37duncvbPAaRmzZrFWB5HCCGEJ554Isb26ILch1qyZInK2fdcdk+wV3G6desWY9nJIQT9/+PChQul/wOqCTMrAC5QrAC4kNTLwAYNGqjx+PHjY2xPkMvT5Rs2bFA5eav80qVLKmfHstOCPSUvp9I2t2nTphivW7cuAGWx77Vc3tmT6L17947xxo0bVe7bb7+N8erVq1XONoeUNzfatGmjcnI5aZeBshmffeftEZ2qxswKgAsUKwAuUKwAuJDUe1b25vjp06djLLt2hhBCkyZNYiyvD9hcz549Vc4eXZA3y+1HTjt16hTjHTt2qJxcz9vfJ/fTALtnJI8rDBgwQOVkl47ly5ernNzDsle6bJdb+X7aYzcFBQUxlu94CCHcdtttMbZdRG233KrGzAqACxQrAC4k9TLQTjN/+umnGNtb5vJ0uT3WIKeyhYWFKie7LISgT5vbE7vt2rUrNQ5BH1fgoxC4HrvUktsWHTp0UDm59LJHHuSWRufOnVXOdlZo2bJljO32ivz/IW+ChKC/N2i/TVjdmFkBcIFiBcAFihUAF5J6z8p+XGHhwoUxvt6PX+2PhuVY3nAP4dq1vvwR7/Wu5mzevFnl5I+RDx48GABJdgO1V1qaN28eY/t+du/ePcZPPfWUysn9VntUwe7pyvdafsjX/v32gxHyXbZXeKobMysALlCsALiQ1MtAu9T7+++/Y2wb7MmjA/aUenp6eoxtczPZ0M+S3RpCCGHWrFkxlkvSEHTzPZrtwZLLtMOHD6vcokWLYmxvZsgjB/L7lyHoTgp2y8IuNeWHH+zRGtl1Ye7cuSq3du3aGNsPqFQ3ZlYAXKBYAXCBYgXAhaTes7LkWtuu++0aXZJ7WBkZGSonOzmEoK83/PDDDyr33Xffxdgeq7DdGwBJfogkPz9f5b766qsY79mzR+X69OkT41atWqmc3AezHye1+7byqow9njB16tQYT5kyReVkF5JE78UyswLgAsUKgAsUKwAuuNqzklcIunbtqnI5OTkxfvDBB1VOnk+x6/Xvv/9ejRcsWBDjbdu2qZw8d5Xo9Tv8smeiZNdZ2xZp5cqVMbZtX2QH3OzsbJWzrWby8vJiPGfOHJWbOXNmjA8cOKByV69evfYfkCDMrAC4QLEC4EJSLwPlNZkQQhg9enSMR40apXKy26Ftnv/JJ5/E+Oeff1Y5+0FUeZNd/rgZqCpyWXjkyBGVk1dc7HGEYcOGxTgrK0vl7J+zdOnSGMuOuyHoJWIyLfssZlYAXKBYAXCBYgXAhYTvWckuhSHojzw+9NBDKnfffffFuH379ionr8m89dZbKvfHH3/E2La54Es0SCay7UsI+ks0ubm5KifbG9k9qmnTpqmxbEOzf/9+lfPyf4CZFQAXKFYAXEjIMlA2z3/kkUdUbty4cTG2P6qVxwq+/vprlZOncH/99VeVsx1HgWQiP+YgPxARQghjxoyJsV0Gyi2Nb775RuVkV9sQdJcQr7cvmFkBcIFiBcAFihUAFxKyZ/XAAw/EeOzYsSonOyTYbqCyQ8LkyZNVTnZEYI8KyUzuUYUQQq9evWJs93BlBxH7IVPZ4XPGjBkqZ6+c1QTMrAC4QLEC4EJCloHy4wrbt29Xua1bt8bYHkGQjcjsiV2vP47FzUGeTJcdQkLQS70RI0aonPzoqe0YsmLFihjXxGWfxcwKgAsUKwAuUKwAuJCQPav169fH2Daol2tveRwhBI4kwC/Z9VYe3QlBdxdp1qyZyi1ZsiTG9uMmf/75Z2U+YtJjZgXABYoVABcSsgw8ffp0qXEIuiMDxxFQU7Rt2zbGmZmZKpeamhrjNWvWqNz06dNjvHnzZpWz3x+s6ZhZAXCBYgXABYoVABcS/sEIi30q1ARy7zUE3eVWftwkBL0XtWzZMpWTV2q8fNihqjCzAuACxQqACykVWXalpKSwRkse60tKSv6T6IeoCar7vW7ZsqUaN2jQIMYFBQUqJzuU3CTKfK+ZWQFwgWIFwAWKFQAXku7oAlDT2X0plA8zKwAuUKwAuFBpy8C0tDQ1LiwsjHFRUVFl/TUAblLMrAC4QLEC4ALFCoALFd2zKggh7C0tcfLkyX//NKiITol+gBqkzPca1a7M97pCdwMBIFFYBgJwgWIFwAWKFQAXKFYAXKBYAXCBYgXABYoVABcoVgBcoFgBcOG/622TqS5QfnEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_aug = {}\n",
    "train_batch = x_pairs['train'][0]\n",
    "train_batch_aug = data_augmentation(train_batch)\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "ax[0, 0].imshow(train_batch_aug[0])\n",
    "ax[0, 1].imshow(train_batch_aug[batch_size])\n",
    "ax[1, 0].imshow(train_batch_aug[1])\n",
    "ax[1, 1].imshow(train_batch_aug[batch_size + 1])\n",
    "for i in range(4):\n",
    "    ax[i // 2, i % 2].set_xticks([])\n",
    "    ax[i // 2, i % 2].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessing = data_augmentation\n",
    "        self.encoder = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(.5),\n",
    "            tf.keras.layers.Dense(128, activation='relu')\n",
    "        ])\n",
    "        self.training_head = tf.keras.layers.Dense(128)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            x = self.preprocessing(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.training_head(x)\n",
    "        x = self.batch_norm(x)\n",
    "        z_i, z_j = tf.split(x, 2, axis=0)\n",
    "        return z_i, z_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z_i, z_j, temperature=1.0):\n",
    "    batch_size = tf.shape(z_i)[0]\n",
    "    labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n",
    "    mask = tf.one_hot(tf.range(batch_size), batch_size)\n",
    "    logits_aa = tf.matmul(z_i, z_i, transpose_b=True) / temperature\n",
    "    logits_aa -= mask * 1e9\n",
    "    logits_bb = tf.matmul(z_j, z_j, transpose_b=True) / temperature\n",
    "    logits_bb -= mask * 1e9\n",
    "    logits_ab = tf.matmul(z_i, z_j, transpose_b=True) / temperature\n",
    "    logits_ba = tf.matmul(z_j, z_i, transpose_b=True) / temperature\n",
    "    loss_a = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([logits_ab, logits_aa], 1))\n",
    "    loss_b = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([logits_ba, logits_bb], 1))\n",
    "    loss = tf.reduce_mean(loss_a + loss_b)\n",
    "    return loss, logits_ab, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 81.79779052734375 Test loss: 10.712457656860352\n",
      "Improved\n",
      "Train loss: 9.735787391662598 Test loss: 7.232767581939697\n",
      "Improved\n",
      "Train loss: 8.840286254882812 Test loss: 5.891907691955566\n",
      "Improved\n",
      "Train loss: 8.324990272521973 Test loss: 6.019481658935547\n",
      "Train loss: 8.067597389221191 Test loss: 5.024805068969727\n",
      "Improved\n",
      "Train loss: 7.72116756439209 Test loss: 4.876263618469238\n",
      "Improved\n",
      "Train loss: 7.2949323654174805 Test loss: 5.502788066864014\n",
      "Train loss: 7.067462921142578 Test loss: 4.799975395202637\n",
      "Improved\n",
      "Train loss: 6.863137722015381 Test loss: 4.994575023651123\n",
      "Train loss: 6.761721134185791 Test loss: 5.309876441955566\n",
      "Train loss: 6.587408065795898 Test loss: 4.64550256729126\n",
      "Improved\n",
      "Train loss: 6.425501346588135 Test loss: 4.278831958770752\n",
      "Improved\n",
      "Train loss: 6.291229724884033 Test loss: 4.322897911071777\n",
      "Train loss: 6.207773685455322 Test loss: 3.9261817932128906\n",
      "Improved\n",
      "Train loss: 5.965686321258545 Test loss: 3.7188169956207275\n",
      "Improved\n",
      "Train loss: 5.778391361236572 Test loss: 3.919776678085327\n",
      "Train loss: 5.536547660827637 Test loss: 3.6614179611206055\n",
      "Improved\n",
      "Train loss: 5.431600093841553 Test loss: 3.1971020698547363\n",
      "Improved\n",
      "Train loss: 5.349613189697266 Test loss: 3.3707752227783203\n",
      "Train loss: 5.243889331817627 Test loss: 3.0853898525238037\n",
      "Improved\n",
      "Train loss: 5.100757122039795 Test loss: 2.7611122131347656\n",
      "Improved\n",
      "Train loss: 4.945746421813965 Test loss: 2.3870177268981934\n",
      "Improved\n",
      "Train loss: 4.705252170562744 Test loss: 2.3237314224243164\n",
      "Improved\n",
      "Train loss: 4.593936443328857 Test loss: 2.1427013874053955\n",
      "Improved\n",
      "Train loss: 4.49872350692749 Test loss: 1.9962328672409058\n",
      "Improved\n",
      "Train loss: 4.452544212341309 Test loss: 1.9373263120651245\n",
      "Improved\n",
      "Train loss: 4.415512561798096 Test loss: 1.9520609378814697\n",
      "Train loss: 4.3504638671875 Test loss: 1.6977803707122803\n",
      "Improved\n",
      "Train loss: 4.252094745635986 Test loss: 1.6697180271148682\n",
      "Improved\n",
      "Train loss: 4.141646862030029 Test loss: 1.6381880044937134\n",
      "Improved\n",
      "Train loss: 4.105792999267578 Test loss: 1.53493070602417\n",
      "Improved\n",
      "Train loss: 4.086762428283691 Test loss: 1.608286738395691\n",
      "Train loss: 4.024212837219238 Test loss: 1.5193402767181396\n",
      "Improved\n",
      "Train loss: 3.9893856048583984 Test loss: 1.611613154411316\n",
      "Train loss: 3.9890339374542236 Test loss: 1.325256586074829\n",
      "Improved\n",
      "Train loss: 3.9536709785461426 Test loss: 1.392637848854065\n",
      "Train loss: 3.9276466369628906 Test loss: 1.3653967380523682\n",
      "Train loss: 3.8821046352386475 Test loss: 1.6018140316009521\n",
      "Train loss: 3.8706798553466797 Test loss: 1.3624087572097778\n",
      "Train loss: 3.8527822494506836 Test loss: 1.3894566297531128\n",
      "Train loss: 3.8106937408447266 Test loss: 1.347861409187317\n",
      "Train loss: 3.7928929328918457 Test loss: 1.3855324983596802\n",
      "Train loss: 3.6924026012420654 Test loss: 1.4836047887802124\n",
      "Train loss: 3.5158755779266357 Test loss: 1.978493571281433\n",
      "Train loss: 3.4700839519500732 Test loss: 1.1279953718185425\n",
      "Improved\n",
      "Train loss: 3.4134154319763184 Test loss: 1.2482223510742188\n",
      "Train loss: 3.3709959983825684 Test loss: 1.1793394088745117\n",
      "Train loss: 3.2874059677124023 Test loss: 0.9589810371398926\n",
      "Improved\n",
      "Train loss: 3.304840087890625 Test loss: 1.1120760440826416\n",
      "Train loss: 3.248413562774658 Test loss: 0.980628490447998\n",
      "Train loss: 3.2263500690460205 Test loss: 1.0156627893447876\n",
      "Train loss: 3.1896660327911377 Test loss: 1.0407109260559082\n",
      "Train loss: 3.1865384578704834 Test loss: 0.8974248170852661\n",
      "Improved\n",
      "Train loss: 3.106910467147827 Test loss: 0.8986569046974182\n",
      "Train loss: 3.11683988571167 Test loss: 0.9940152168273926\n",
      "Train loss: 3.0876071453094482 Test loss: 0.914982795715332\n",
      "Train loss: 3.033256769180298 Test loss: 0.863480806350708\n",
      "Improved\n",
      "Train loss: 2.9976749420166016 Test loss: 0.8903393745422363\n",
      "Train loss: 2.990626573562622 Test loss: 0.8473498821258545\n",
      "Improved\n",
      "Train loss: 2.96563458442688 Test loss: 0.9023417234420776\n",
      "Train loss: 2.936744213104248 Test loss: 0.8633829951286316\n",
      "Train loss: 2.892674446105957 Test loss: 0.7164841890335083\n",
      "Improved\n",
      "Train loss: 2.9178879261016846 Test loss: 0.7086921334266663\n",
      "Improved\n",
      "Train loss: 2.8589577674865723 Test loss: 0.7363213896751404\n",
      "Train loss: 2.8388428688049316 Test loss: 0.7686760425567627\n",
      "Train loss: 2.808168411254883 Test loss: 0.6928267478942871\n",
      "Improved\n",
      "Train loss: 2.819117307662964 Test loss: 0.7782696485519409\n",
      "Train loss: 2.828906774520874 Test loss: 0.7267235517501831\n",
      "Train loss: 2.7888617515563965 Test loss: 0.7519715428352356\n",
      "Train loss: 2.7856740951538086 Test loss: 0.7945308685302734\n",
      "Train loss: 2.773803472518921 Test loss: 0.7083196640014648\n",
      "Train loss: 2.764056444168091 Test loss: 0.7283822298049927\n",
      "Train loss: 2.7321479320526123 Test loss: 0.6455267071723938\n",
      "Improved\n",
      "Train loss: 2.710482358932495 Test loss: 0.7106349468231201\n",
      "Train loss: 2.6946213245391846 Test loss: 0.726405918598175\n",
      "Train loss: 2.7092087268829346 Test loss: 0.6509488224983215\n",
      "Train loss: 2.6726207733154297 Test loss: 0.6178946495056152\n",
      "Improved\n",
      "Train loss: 2.675485134124756 Test loss: 0.6921477317810059\n",
      "Train loss: 2.657824993133545 Test loss: 0.6995837092399597\n",
      "Train loss: 2.6304826736450195 Test loss: 0.6249732971191406\n",
      "Train loss: 2.632469654083252 Test loss: 0.6792424321174622\n",
      "Train loss: 2.6341097354888916 Test loss: 0.6948114037513733\n",
      "Train loss: 2.6089391708374023 Test loss: 0.6374177932739258\n",
      "Train loss: 2.5953445434570312 Test loss: 0.5684035420417786\n",
      "Improved\n",
      "Train loss: 2.567274332046509 Test loss: 0.602735161781311\n",
      "Train loss: 2.542160749435425 Test loss: 0.649488091468811\n",
      "Train loss: 2.542419910430908 Test loss: 0.6501227021217346\n",
      "Train loss: 2.5327401161193848 Test loss: 0.6357584595680237\n",
      "Train loss: 2.5290186405181885 Test loss: 0.6135529279708862\n",
      "Train loss: 2.494494915008545 Test loss: 0.5185662508010864\n",
      "Improved\n",
      "Train loss: 2.4894869327545166 Test loss: 0.5732099413871765\n",
      "Train loss: 2.471811294555664 Test loss: 0.5762491822242737\n",
      "Train loss: 2.4677069187164307 Test loss: 0.5088339447975159\n",
      "Improved\n",
      "Train loss: 2.477938175201416 Test loss: 0.5542024374008179\n",
      "Train loss: 2.4605278968811035 Test loss: 0.5839177370071411\n",
      "Train loss: 2.4498445987701416 Test loss: 0.5761932730674744\n",
      "Train loss: 2.4371702671051025 Test loss: 0.542393684387207\n",
      "Train loss: 2.431732416152954 Test loss: 0.5062463283538818\n",
      "Improved\n",
      "Train loss: 2.4087629318237305 Test loss: 0.5056391954421997\n",
      "Improved\n",
      "Train loss: 2.4041335582733154 Test loss: 0.5383116602897644\n",
      "Train loss: 2.3872547149658203 Test loss: 0.5193419456481934\n",
      "Train loss: 2.35953688621521 Test loss: 0.5150343179702759\n",
      "Train loss: 2.384760618209839 Test loss: 0.5298417806625366\n",
      "Train loss: 2.3571836948394775 Test loss: 0.47804582118988037\n",
      "Improved\n",
      "Train loss: 2.3579323291778564 Test loss: 0.45325812697410583\n",
      "Improved\n",
      "Train loss: 2.3456811904907227 Test loss: 0.49276772141456604\n",
      "Train loss: 2.3404669761657715 Test loss: 0.48950478434562683\n",
      "Train loss: 2.326463460922241 Test loss: 0.47627922892570496\n",
      "Train loss: 2.337858200073242 Test loss: 0.5003957152366638\n",
      "Train loss: 2.303901433944702 Test loss: 0.5322108268737793\n",
      "Train loss: 2.2933201789855957 Test loss: 0.45112520456314087\n",
      "Improved\n",
      "Train loss: 2.309406042098999 Test loss: 0.472857266664505\n",
      "Train loss: 2.2874605655670166 Test loss: 0.4609183669090271\n",
      "Train loss: 2.277139663696289 Test loss: 0.46681687235832214\n",
      "Train loss: 2.28487491607666 Test loss: 0.4614448547363281\n",
      "Train loss: 2.271451950073242 Test loss: 0.493441641330719\n",
      "Train loss: 2.266284465789795 Test loss: 0.448941707611084\n",
      "Improved\n",
      "Train loss: 2.2460248470306396 Test loss: 0.4490797221660614\n",
      "Train loss: 2.2252039909362793 Test loss: 0.46132150292396545\n",
      "Train loss: 2.2567126750946045 Test loss: 0.4535787105560303\n",
      "Train loss: 2.2380990982055664 Test loss: 0.4636063575744629\n",
      "Train loss: 2.229708433151245 Test loss: 0.4744336009025574\n",
      "Train loss: 2.2395808696746826 Test loss: 0.41448652744293213\n",
      "Improved\n",
      "Train loss: 2.189232110977173 Test loss: 0.4348166584968567\n",
      "Train loss: 2.216198444366455 Test loss: 0.4333566427230835\n",
      "Train loss: 2.1993792057037354 Test loss: 0.4074762761592865\n",
      "Improved\n",
      "Train loss: 2.1907901763916016 Test loss: 0.43311262130737305\n",
      "Train loss: 2.19805908203125 Test loss: 0.45317795872688293\n",
      "Train loss: 2.204864740371704 Test loss: 0.4522503614425659\n",
      "Train loss: 2.167588472366333 Test loss: 0.43865227699279785\n",
      "Train loss: 2.186406373977661 Test loss: 0.4324661195278168\n",
      "Train loss: 2.1731204986572266 Test loss: 0.4463118016719818\n",
      "Train loss: 2.1652426719665527 Test loss: 0.4487876296043396\n",
      "Train loss: 2.171877145767212 Test loss: 0.38608717918395996\n",
      "Improved\n",
      "Train loss: 2.171433687210083 Test loss: 0.4850817918777466\n",
      "Train loss: 2.149712085723877 Test loss: 0.4393134117126465\n",
      "Train loss: 2.168898820877075 Test loss: 0.45287245512008667\n",
      "Train loss: 2.1294071674346924 Test loss: 0.4426727294921875\n",
      "Train loss: 2.1723387241363525 Test loss: 0.47071436047554016\n",
      "Train loss: 2.1450631618499756 Test loss: 0.4485939145088196\n",
      "Train loss: 2.1439828872680664 Test loss: 0.4028363525867462\n",
      "Train loss: 2.1428749561309814 Test loss: 0.4012070298194885\n",
      "Train loss: 2.142028331756592 Test loss: 0.4644313156604767\n",
      "Train loss: 2.1466453075408936 Test loss: 0.4091574549674988\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "temperature = .1\n",
    "model = ContrastiveModel()\n",
    "best_test_loss = float('inf')\n",
    "k = 0\n",
    "patience = 10\n",
    "for epoch in range(999):\n",
    "    train_loss_avg = tf.keras.metrics.Mean()\n",
    "    test_loss_avg = tf.keras.metrics.Mean()\n",
    "    for batch in x_pairs['train']:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_i, z_j = model(batch, training=True)\n",
    "            loss, _, _ = contrastive_loss(z_i, z_j, temperature=temperature)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            train_loss_avg.update_state(loss)\n",
    "    for batch in x_pairs['test']:\n",
    "        z_i, z_j = model(batch, training=False)\n",
    "        loss, _, _ = contrastive_loss(z_i, z_j, temperature=temperature)\n",
    "        test_loss_avg.update_state(loss)\n",
    "    train_loss = train_loss_avg.result()\n",
    "    test_loss = test_loss_avg.result()\n",
    "    print(f'Train loss: {train_loss} Test loss: {test_loss}')\n",
    "    if test_loss < best_test_loss:\n",
    "        k = 0\n",
    "        best_test_loss = test_loss\n",
    "        print('Improved')\n",
    "        model.save_weights('contrastive_weights.h5')\n",
    "    else:\n",
    "        k += 1\n",
    "        if k >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.38608718\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('contrastive_weights.h5')\n",
    "test_loss_avg = tf.keras.metrics.Mean()\n",
    "for batch in x_pairs['test']:\n",
    "    z_i, z_j = model(batch)\n",
    "    loss, _, _ = contrastive_loss(z_i, z_j, temperature=temperature)\n",
    "    test_loss_avg.update_state(loss)\n",
    "print('Test loss: ' + str(test_loss_avg.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train supervised head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveProbe(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.trainable = False\n",
    "        self.supervised_head = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.model.encoder(x)\n",
    "        return self.supervised_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_probe = {}\n",
    "for type in ['train', 'test']:\n",
    "    x_probe[type] = tf.repeat(x[type][:, :, :, tf.newaxis], 3, axis=-1)\n",
    "    x_probe[type] = rescale(x_probe[type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 1.8018 - accuracy: 0.4316 - val_loss: 0.7065 - val_accuracy: 0.8706\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7262 - accuracy: 0.8226 - val_loss: 0.4733 - val_accuracy: 0.8991\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5604 - accuracy: 0.8517 - val_loss: 0.3830 - val_accuracy: 0.9111\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4979 - accuracy: 0.8632 - val_loss: 0.3396 - val_accuracy: 0.9166\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4555 - accuracy: 0.8674 - val_loss: 0.3086 - val_accuracy: 0.9204\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4232 - accuracy: 0.8753 - val_loss: 0.2839 - val_accuracy: 0.9254\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4130 - accuracy: 0.8777 - val_loss: 0.2700 - val_accuracy: 0.9270\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3903 - accuracy: 0.8819 - val_loss: 0.2601 - val_accuracy: 0.9280\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3870 - accuracy: 0.8821 - val_loss: 0.2525 - val_accuracy: 0.9296\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3829 - accuracy: 0.8805 - val_loss: 0.2443 - val_accuracy: 0.9318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d1963c77c0>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised = ContrastiveProbe(model)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint('contrastive_probe.h5', save_best_only=True)\n",
    "]\n",
    "supervised.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "supervised.fit(x_probe['train'], y['train'], validation_data=(x_probe['test'], y['test']), batch_size=batch_size, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2443 - accuracy: 0.9318\n",
      "Accuracy: 0.932\n"
     ]
    }
   ],
   "source": [
    "supervised.load_weights('contrastive_probe.h5')\n",
    "loss, acc = supervised.evaluate(x_probe['test'], y['test'])\n",
    "print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also try randomly initialized encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 2.2064 - accuracy: 0.2434 - val_loss: 1.9387 - val_accuracy: 0.7226\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.8800 - accuracy: 0.5225 - val_loss: 1.6868 - val_accuracy: 0.7671\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.6773 - accuracy: 0.5579 - val_loss: 1.5080 - val_accuracy: 0.7762\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5442 - accuracy: 0.5655 - val_loss: 1.3809 - val_accuracy: 0.7837\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4496 - accuracy: 0.5796 - val_loss: 1.2845 - val_accuracy: 0.7879\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3815 - accuracy: 0.5869 - val_loss: 1.2113 - val_accuracy: 0.7934\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3374 - accuracy: 0.5914 - val_loss: 1.1502 - val_accuracy: 0.7949\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3116 - accuracy: 0.5861 - val_loss: 1.1059 - val_accuracy: 0.7945\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2770 - accuracy: 0.5935 - val_loss: 1.0633 - val_accuracy: 0.8002\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2560 - accuracy: 0.5945 - val_loss: 1.0363 - val_accuracy: 0.7987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d1a4517100>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import tensorflow.keras.backend as K\n",
    "rand_model = ContrastiveModel()\n",
    "rand_probe = ContrastiveProbe(rand_model)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint('contrastive_probe_randomly_initialized.h5', save_best_only=True)\n",
    "]\n",
    "rand_probe.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rand_probe.fit(x_probe['train'], y['train'], validation_data=(x_probe['test'], y['test']), batch_size=batch_size, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0363 - accuracy: 0.7987\n",
      "Accuracy: 0.799\n"
     ]
    }
   ],
   "source": [
    "rand_probe.load_weights('contrastive_probe_randomly_initialized.h5')\n",
    "loss, acc = rand_probe.evaluate(x_probe['test'], y['test'])\n",
    "print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also try unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['contrastive_model_17/dense_103/kernel:0', 'contrastive_model_17/dense_103/bias:0', 'contrastive_model_17/batch_normalization_16/gamma:0', 'contrastive_model_17/batch_normalization_16/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['contrastive_model_17/dense_103/kernel:0', 'contrastive_model_17/dense_103/bias:0', 'contrastive_model_17/batch_normalization_16/gamma:0', 'contrastive_model_17/batch_normalization_16/beta:0'] when minimizing the loss.\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.9965 - accuracy: 0.6808 - val_loss: 0.0945 - val_accuracy: 0.9701\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1544 - accuracy: 0.9508 - val_loss: 0.0597 - val_accuracy: 0.9815\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1196 - accuracy: 0.9620 - val_loss: 0.0522 - val_accuracy: 0.9825\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0931 - accuracy: 0.9707 - val_loss: 0.0422 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0869 - accuracy: 0.9709 - val_loss: 0.0368 - val_accuracy: 0.9883\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0775 - accuracy: 0.9762 - val_loss: 0.0397 - val_accuracy: 0.9865\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0678 - accuracy: 0.9787 - val_loss: 0.0331 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0669 - accuracy: 0.9782 - val_loss: 0.0298 - val_accuracy: 0.9905\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0607 - accuracy: 0.9797 - val_loss: 0.0297 - val_accuracy: 0.9902\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0590 - accuracy: 0.9813 - val_loss: 0.0299 - val_accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf341b3f10>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfrozen_model = ContrastiveModel()\n",
    "unfrozen_probe = ContrastiveProbe(model)\n",
    "unfrozen_probe.model.trainable = True\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint('contrastive_probe_unfrozen.h5', save_best_only=True)\n",
    "]\n",
    "unfrozen_probe.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "unfrozen_probe.fit(x_probe['train'], y['train'], validation_data=(x_probe['test'], y['test']), batch_size=batch_size, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0352 - accuracy: 0.9891\n",
      "Accuracy: 0.989\n"
     ]
    }
   ],
   "source": [
    "unfrozen_probe.load_weights('contrastive_probe_unfrozen.h5')\n",
    "loss, acc = unfrozen_probe.evaluate(x_probe['test'], y['test'])\n",
    "print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why is random initialization accuracy so high?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu')\n",
    "        ])\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular training accuracy should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 4.2513 - accuracy: 0.8266 - val_loss: 0.5034 - val_accuracy: 0.9164\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3702 - accuracy: 0.9276 - val_loss: 0.2621 - val_accuracy: 0.9374\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2065 - accuracy: 0.9461 - val_loss: 0.2009 - val_accuracy: 0.9449\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1575 - accuracy: 0.9571 - val_loss: 0.1874 - val_accuracy: 0.9546\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1469 - accuracy: 0.9586 - val_loss: 0.1748 - val_accuracy: 0.9552\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1212 - accuracy: 0.9660 - val_loss: 0.1367 - val_accuracy: 0.9646\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1173 - accuracy: 0.9690 - val_loss: 0.1471 - val_accuracy: 0.9598\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0975 - accuracy: 0.9720 - val_loss: 0.1530 - val_accuracy: 0.9634\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1001 - accuracy: 0.9737 - val_loss: 0.1561 - val_accuracy: 0.9633\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0823 - accuracy: 0.9781 - val_loss: 0.1636 - val_accuracy: 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf49496670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestModel()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x['train'], y['train'], validation_data=(x['test'], y['test']), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen xavier initialized encoder accuracy should be lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 19.5067 - accuracy: 0.5377 - val_loss: 2.9179 - val_accuracy: 0.7918\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.4774 - accuracy: 0.7942 - val_loss: 1.6057 - val_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5255 - accuracy: 0.8005 - val_loss: 1.1928 - val_accuracy: 0.8111\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.1500 - accuracy: 0.8015 - val_loss: 0.9701 - val_accuracy: 0.8065\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0135 - accuracy: 0.7985 - val_loss: 0.8489 - val_accuracy: 0.8219\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9866 - accuracy: 0.7948 - val_loss: 1.0050 - val_accuracy: 0.7917\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9825 - accuracy: 0.7924 - val_loss: 0.8673 - val_accuracy: 0.8215\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9487 - accuracy: 0.7958 - val_loss: 0.9009 - val_accuracy: 0.8092\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9431 - accuracy: 0.7968 - val_loss: 0.8720 - val_accuracy: 0.8081\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9622 - accuracy: 0.7928 - val_loss: 0.7572 - val_accuracy: 0.8357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf6129f160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestModel()\n",
    "model.encoder.trainable = False\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x['train'], y['train'], validation_data=(x['test'], y['test']), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen zero initialized encoder accuracy should be random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3019 - accuracy: 0.1078 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3011 - accuracy: 0.1131 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3015 - accuracy: 0.1127 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3016 - accuracy: 0.1118 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3008 - accuracy: 0.1154 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3016 - accuracy: 0.1104 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3016 - accuracy: 0.1121 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1117 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1132 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d1177c0460>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestModel()\n",
    "model(x['train'])\n",
    "for layer in model.encoder.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) > 0:\n",
    "        w, b = weights\n",
    "        layer.set_weights([tf.zeros(w.shape), tf.zeros(b.shape)])\n",
    "model.encoder.trainable = False\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x['train'], y['train'], validation_data=(x['test'], y['test']), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
